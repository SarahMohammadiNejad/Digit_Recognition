{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image # To open the images\n",
    "import os #Use to find the name of the files (here images) in a directory\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Activation, Flatten\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   199\n",
      "1   199\n",
      "2   199\n",
      "3   199\n",
      "4   199\n",
      "5   199\n",
      "6   199\n",
      "7   199\n",
      "8   199\n",
      "9   199\n"
     ]
    }
   ],
   "source": [
    "# Read images and convert to np.array make lists. \n",
    "#Images (image_array_list) for later X and the digit written in each image (image_value_list) for y.  \n",
    "\n",
    "image_list = []\n",
    "image_array_list = []\n",
    "image_value_list = []\n",
    "\n",
    "objects = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "for ob_idx, ob in enumerate(objects):\n",
    "    path = \"datadig/\"+ob\n",
    "\n",
    "    obj_name_list = os.listdir(path)\n",
    "\n",
    "    for p_idx,p in enumerate(obj_name_list[0:200]):\n",
    "\n",
    "        im = Image.open(path+\"/\"+p)\n",
    "\n",
    "        image_list.append(im)\n",
    "        image_array_list.append(np.array(im))\n",
    "        image_value_list.append(ob_idx)\n",
    "    print(f'{ob}   {p_idx}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAYAAAByDd+UAAABFElEQVR4nO3UvUoDQRSG4SeiEkQ7BVsRxJuxUS/B1j6Cor2lrXgHgq14AV6Bha3YWIlG4080WswsWWQ2IXFSJR8suzvf7Hn3nDMzTDTROGgDZ9gdNWgbLfyUrm/MjgK2g06EdCKoeG/mhjVKwZ9RK3nF+FouWE03q/2EfxT9+1zAed1eVekFT1Xm1IDAdgR+9JhzEu8rA8ZOak4o51uPOevRb+QAwjse+/zUFy5T5qAlhbs+flHuhVzAC2Fzr1b408JqnhkidlLLeMVxhX8g9HkvFxAecFXhtYTVnKzeMCWFW+kenaOOayHLbDoVMjksjW3qnqu11Ef/UR2fMXj7z/NWblihRaG0TeEou8HSqGATjbF+AeOAQtZVV5wyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=28x28>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the matrix size of image array is: (28, 28, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAAANXklEQVR4nO3db6xU9Z3H8c9HFiQKJiA3hIAuLBETs7HQTMgmkMZNs436BKqJKQ8qmzS5faCkxiZdwpoAPiK63boPtIZaUnZTxcbKVhOzW5agpk8aB8MqavyPKYgwLom9JGoFvvvgHswV75y5zJyZOdzv+5VMZuZ859zzdfTjmTm/OefniBCA6e+yYTcAYDAIO5AEYQeSIOxAEoQdSOKvBrmxBQsWxNKlSwe5SSCVgwcPfhwRI5PVegq77Zsl/ZukGZIei4gdZa9funSpms1mL5sEUML2B+1qXX+Mtz1D0sOSbpF0g6QNtm/o9u8B6K9evrOvlvRORLwXEX+RtEfSumraAlC1XsK+WNKfJjw/Wiz7Ctujtpu2m61Wq4fNAehF34/GR8TOiGhERGNkZNLjBgAGoJewH5N0zYTnS4plAGqol7C/JOk628tsz5L0PUnPVNMWgKp1PfQWEWds3y3pvzU+9LYrIl6rrDMAleppnD0inpP0XEW9AOgjfi4LJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKKnKZttH5E0JumspDMR0aiiKQDV6ynshb+PiI8r+DsA+oiP8UASvYY9JP3e9kHbo5O9wPao7abtZqvV6nFzALrVa9jXRsQ3Jd0i6S7b37rwBRGxMyIaEdEYGRnpcXMAutVT2CPiWHF/UtJeSauraApA9boOu+0rbc89/1jSdyQdrqoxANXq5Wj8Qkl7bZ//O49HxH9V0hWmjWeffbZtbe/evaXrrlq1qrS+adOmrnrKquuwR8R7kr5RYS8A+oihNyAJwg4kQdiBJAg7kARhB5JwRAxsY41GI5rN5sC2h9499dRTpfU777yztP7pp59W2c5XXHZZ+b6qbNuzZs2qup1asH2w3dmn7NmBJAg7kARhB5Ig7EAShB1IgrADSRB2IIkqLjiJS9hjjz1WWh8dnfRqY1/q9DuN4hToi65N5W+fO3eutH711Ve3rY2NjZWuOx2xZweSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBhnn+Z27NhRWt+yZUtpvdNY99y5c0vrn3zySdtap3H2Tjqdz3769Om2tbfeeqt03RUrVnTVU52xZweSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJLhu/DRQ9u+w01h0p7Hu+++/v7R+3333ldb7adu2baX17du3t60tXry4dN2jR49209LQ9XTdeNu7bJ+0fXjCsvm299l+u7ifV2XDAKo3lY/xv5J08wXLNkvaHxHXSdpfPAdQYx3DHhEvSjp1weJ1knYXj3dLWl9tWwCq1u0BuoURcbx4/JGkhe1eaHvUdtN2s9Vqdbk5AL3q+Wh8jB8danuEKCJ2RkQjIhojIyO9bg5Al7oN+wnbiySpuD9ZXUsA+qHbsD8jaWPxeKOk31XTDoB+6TjObvsJSTdJWiDphKStkv5T0m8kXSvpA0l3RMSFB/G+hnH2/ig7b7vT+eadxuHPnj3bVU91MGfOnLa1GTNmlK5bdh5+nZWNs3e8eEVEbGhT+nZPXQEYKH4uCyRB2IEkCDuQBGEHkiDsQBJcSnoamDlzZttap1NYL7/88qrbqY1Nmza1rT3yyCOl677//vul9WXLlnXV0zCxZweSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBhnnwZ6OQ11kJcSH7SNGze2rT300EOl6z755JOl9c2bL71rrLJnB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkGGefBq644oq2tVmzZpWuO3v27KrbqY1rr722be2LL74oXffAgQOldcbZAdQWYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTj7NFc21ixJrVZrQJ0MXi/XxB8bG6uwk3rouGe3vcv2SduHJyzbZvuY7UPF7db+tgmgV1P5GP8rSTdPsvxnEbGyuD1XbVsAqtYx7BHxoqRTA+gFQB/1coDubtuvFB/z57V7ke1R203bzen8/RCou27D/nNJyyWtlHRc0k/bvTAidkZEIyIaIyMjXW4OQK+6CntEnIiIsxFxTtIvJK2uti0AVesq7LYXTXj6XUmH270WQD10HGe3/YSkmyQtsH1U0lZJN9leKSkkHZH0w/61iF6sX7++tP7www+X1t99993S+vLlyy+2pYE5c+ZM21qn6+V3Ot/9UtQx7BGxYZLFv+xDLwD6iJ/LAkkQdiAJwg4kQdiBJAg7kASnuE5z9957b2m909Dbo48+Wlp/8MEHL7qnQXnggQfa1joNvd12221VtzN07NmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAl3Gm+sUqPRiGazObDtobOFCxeW1m+88cbS+r59+6psp1JlU1l3OoX1888/L61fdlk995O2D0ZEY7JaPTsGUDnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC89mTu/7660vrdZ66+Pbbby+tf/bZZ21ra9euLV23ruPovZh+/0QAJkXYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzp7cihUrSuuPP/54aX379u2l9a1bt150T+c9/fTTpfW9e/eW1m23rb3wwgtd9XQp67hnt32N7QO2X7f9mu0fFcvn295n++3ifl7/2wXQral8jD8j6ccRcYOkv5N0l+0bJG2WtD8irpO0v3gOoKY6hj0ijkfEy8XjMUlvSFosaZ2k3cXLdkta36ceAVTgog7Q2V4qaZWkP0paGBHHi9JHkia9mJntUdtN281Wq9VLrwB6MOWw254j6beS7omIP0+sxfhVKye9cmVE7IyIRkQ0RkZGemoWQPemFHbbMzUe9F9HxPlDpCdsLyrqiySd7E+LAKrQ8VLSHh+/2C3pVETcM2H5g5L+LyJ22N4saX5E/KTsb3Ep6fopOw1Ukq666qrS+tmzZ0vrZaeKlg2N9fq3JWnPnj1ta51Oj71UlV1Keirj7GskfV/Sq7YPFcu2SNoh6Te2fyDpA0l3VNArgD7pGPaI+IOkdv8L/na17QDoF34uCyRB2IEkCDuQBGEHkiDsQBKc4prc7NmzS+sffvhhaX3NmjVdr99pnHzJkiWl9eeff760zi82v4o9O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTg7Si1YsKC0/uabbw6oE/SKPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4k0THstq+xfcD267Zfs/2jYvk228dsHyput/a/XQDdmsrFK85I+nFEvGx7rqSDtvcVtZ9FxL/0rz0AVZnK/OzHJR0vHo/ZfkPS4n43BqBaF/Wd3fZSSask/bFYdLftV2zvsj2vzTqjtpu2m61Wq7duAXRtymG3PUfSbyXdExF/lvRzScslrdT4nv+nk60XETsjohERDebeAoZnSmG3PVPjQf91RDwtSRFxIiLORsQ5Sb+QtLp/bQLo1VSOxlvSLyW9ERH/OmH5ogkv+66kw9W3B6AqUzkav0bS9yW9avtQsWyLpA22V0oKSUck/bAP/QGoyFSOxv9BkicpPVd9OwD6hV/QAUkQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHknBEDG5jdkvSBxMWLZD08cAauDh17a2ufUn01q0qe/vriJj0+m8DDfvXNm43I6IxtAZK1LW3uvYl0Vu3BtUbH+OBJAg7kMSww75zyNsvU9fe6tqXRG/dGkhvQ/3ODmBwhr1nBzAghB1IYihht32z7Tdtv2N78zB6aMf2EduvFtNQN4fcyy7bJ20fnrBsvu19tt8u7iedY29IvdViGu+SacaH+t4Ne/rzgX9ntz1D0luS/kHSUUkvSdoQEa8PtJE2bB+R1IiIof8Aw/a3JJ2W9O8R8bfFsgcknYqIHcX/KOdFxD/VpLdtkk4PexrvYraiRROnGZe0XtI/aojvXUlfd2gA79sw9uyrJb0TEe9FxF8k7ZG0bgh91F5EvCjp1AWL10naXTzerfH/WAauTW+1EBHHI+Ll4vGYpPPTjA/1vSvpayCGEfbFkv404flR1Wu+95D0e9sHbY8Ou5lJLIyI48XjjyQtHGYzk+g4jfcgXTDNeG3eu26mP+8VB+i+bm1EfFPSLZLuKj6u1lKMfwer09jplKbxHpRJphn/0jDfu26nP+/VMMJ+TNI1E54vKZbVQkQcK+5PStqr+k1FfeL8DLrF/ckh9/OlOk3jPdk046rBezfM6c+HEfaXJF1ne5ntWZK+J+mZIfTxNbavLA6cyPaVkr6j+k1F/YykjcXjjZJ+N8RevqIu03i3m2ZcQ37vhj79eUQM/CbpVo0fkX9X0j8Po4c2ff2NpP8tbq8NuzdJT2j8Y90XGj+28QNJV0vaL+ltSf8jaX6NevsPSa9KekXjwVo0pN7Wavwj+iuSDhW3W4f93pX0NZD3jZ/LAklwgA5IgrADSRB2IAnCDiRB2IEkCDuQBGEHkvh/eLY8qTOSpMgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(image_array_list[45]) \n",
    "print(f'the matrix size of image array is: {image_array_list[45].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of read image array                         (2000, 28, 28, 4)\n",
      "size of image array after choosing 1 color layer (2000, 28, 28)\n",
      "size of image array after reshape                (2000, 28, 28, 1)\n",
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert lists of X and y to arrays:\n",
    "Xtemp= np.array(image_array_list)\n",
    "y= np.array(image_value_list)\n",
    "\n",
    "# !The color is just in layer 3. Layer 0-2 has nothing\n",
    "X = Xtemp[:,:,:,3]\n",
    "\n",
    "print(f'size of read image array                         {Xtemp.shape}')\n",
    "print(f'size of image array after choosing 1 color layer {X.shape}')\n",
    "\n",
    "# For image recognition using CNN we need 4th component for colors (use reshape)  \n",
    "X = X.reshape(X.shape[0], 28, 28, 1)\n",
    "print(f'size of image array after reshape                {X.shape}')\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(X[45])\n",
    "print(y[45])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting my CNN fit, I found that the model does not work good for some digits (with very low scores in test set). I concluded this can be related to train-test splitting and maybe I need to shuffle the images before splitting, otherwise the model is not trained correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shuffle X and y arrays at the same time (with the same order)\n",
    "def shuffel_arrays(arr1, arr2):\n",
    "    assert len(arr1) == len(arr2)\n",
    "    p = np.random.permutation(len(arr1))\n",
    "    return arr1[p], arr2[p]\n",
    "\n",
    "# Calling this function\n",
    "X_sh,y_sh = shuffel_arrays(X,y)\n",
    "X = X_sh\n",
    "y = y_sh\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(X[45])\n",
    "print(y[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the shape of y? (2000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "print(f'what is the shape of y? {y.shape}')\n",
    "\n",
    "# Convert digits(0-9) in y to 10 cells with values=0|1\n",
    "ycat = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before y Categorization    X: (2000, 28, 28, 1), and y: (2000,)\n",
      "After y Categorization     X: (2000, 28, 28, 1), and y: (2000, 10)\n",
      "---------------\n",
      "After categorization and split: \n",
      "Shape of train X: (1400, 28, 28, 1), and y: (1400, 10)\n",
      "Shape of test X: (600, 28, 28, 1), and y: (600, 10)\n"
     ]
    }
   ],
   "source": [
    "#Some prints to check the input (X) and outpus (y) for CNN.\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, ycat, test_size=0.3, random_state=42)\n",
    "print(f'Before y Categorization    X: {X.shape}, and y: {y.shape}')\n",
    "print(f'After y Categorization     X: {X.shape}, and y: {ycat.shape}')\n",
    "print('---------------')\n",
    "print('After categorization and split: ') \n",
    "print(f'Shape of train X: {Xtrain.shape}, and y: {ytrain.shape}')\n",
    "print(f'Shape of test X: {Xtest.shape}, and y: {ytest.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model for feature learning (Conv2D and Maxpooling) \n",
    "# Then flatten and then Dense layers\n",
    "K.clear_session()\n",
    "model = Sequential([\n",
    "\n",
    "    Conv2D(6, kernel_size=(5,5), strides=(1,1),\n",
    "           activation='relu', input_shape=Xtrain[0].shape,\n",
    "           padding='valid'),\n",
    "    \n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'),\n",
    "    \n",
    "    \n",
    "    Conv2D(16, kernel_size=(5,5),strides=(1,1),\n",
    "           activation='relu', padding='valid'),\n",
    "    \n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'),\n",
    "    \n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(120, activation='relu'),\n",
    "    \n",
    "    Dense(86, activation='relu'),\n",
    "    \n",
    "    Dense(len(objects), activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define comile and its parameters\n",
    "model.compile(optimizer='rmsprop', # type of Gradient Descent 'rmsprop'\n",
    "              loss='categorical_crossentropy', # multiclass: 'categorical_crossentropy'\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 24, 24, 6)         156       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 12, 12, 6)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 16)          2416      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 4, 4, 16)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               30840     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 86)                10406     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                870       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44688 (174.56 KB)\n",
      "Trainable params: 44688 (174.56 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.keras as keras\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss', #the thing we are monitoring\n",
    "    min_delta = 0.0005, #the minimum change in the quantity that we want for the model to train for another epoch\n",
    "    patience = 3, #number of epochs with no improvement needed for the model to stop\n",
    "    verbose = 1, #0 is silent, 1 means a message is displayed when something happens\n",
    "    mode = 'auto'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "8/8 [==============================] - 1s 61ms/step - loss: 18.0689 - accuracy: 0.2313 - val_loss: 4.7021 - val_accuracy: 0.3821\n",
      "Epoch 2/500\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 3.3204 - accuracy: 0.4616 - val_loss: 2.5284 - val_accuracy: 0.5571\n",
      "Epoch 3/500\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.7340 - accuracy: 0.6313 - val_loss: 1.5923 - val_accuracy: 0.6500\n",
      "Epoch 4/500\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.8099 - accuracy: 0.7902 - val_loss: 1.3894 - val_accuracy: 0.7000\n",
      "Epoch 5/500\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.8318 - accuracy: 0.7839 - val_loss: 0.7440 - val_accuracy: 0.8107\n",
      "Epoch 6/500\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.2280 - accuracy: 0.9312 - val_loss: 0.7612 - val_accuracy: 0.8250\n",
      "Epoch 7/500\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.1256 - accuracy: 0.9625 - val_loss: 0.5697 - val_accuracy: 0.8464\n",
      "Epoch 8/500\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0595 - accuracy: 0.9821 - val_loss: 0.4842 - val_accuracy: 0.8893\n",
      "Epoch 9/500\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0281 - accuracy: 0.9937 - val_loss: 0.4240 - val_accuracy: 0.8964\n",
      "Epoch 10/500\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.7380 - accuracy: 0.8420 - val_loss: 0.6716 - val_accuracy: 0.8571\n",
      "Epoch 11/500\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0471 - accuracy: 0.9821 - val_loss: 0.3788 - val_accuracy: 0.9143\n",
      "Epoch 12/500\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0241 - accuracy: 0.9920 - val_loss: 0.2272 - val_accuracy: 0.9429\n",
      "Epoch 13/500\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2382 - val_accuracy: 0.9429\n",
      "Epoch 14/500\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2335 - val_accuracy: 0.9429\n",
      "Epoch 15/500\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 9.5414e-04 - accuracy: 1.0000 - val_loss: 0.2392 - val_accuracy: 0.9429\n",
      "Epoch 15: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9ccd932b80>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model learning, it will stop when conditions in early stop are fullfilled\n",
    "model.fit(Xtrain, ytrain, epochs=500, batch_size=150, validation_split = 0.2,callbacks = [early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict the test set\n",
    "ytest_pred = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In test dataset the total number of correct guessess is 563 out of 600\n",
      "score = 0.93833\n",
      "----------------\n",
      "The % of being wrong for each object:\n",
      "\n",
      "0 is guessed wrongly 7.0 times out of 59.0 --> 12%\n",
      "1 is guessed wrongly 2.0 times out of 60.0 --> 3%\n",
      "2 is guessed wrongly 1.0 times out of 54.0 --> 2%\n",
      "3 is guessed wrongly 6.0 times out of 51.0 --> 12%\n",
      "4 is guessed wrongly 2.0 times out of 61.0 --> 3%\n",
      "5 is guessed wrongly 6.0 times out of 59.0 --> 10%\n",
      "6 is guessed wrongly 2.0 times out of 63.0 --> 3%\n",
      "7 is guessed wrongly 6.0 times out of 69.0 --> 9%\n",
      "8 is guessed wrongly 5.0 times out of 60.0 --> 8%\n",
      "9 is guessed wrongly 0.0 times out of 64.0 --> 0%\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy of the model for each digit separately\n",
    "\n",
    "num_Obj=len(objects)\n",
    "wrong_count_obj = np.zeros((num_Obj,1))\n",
    "tot_count_obj = np.zeros((num_Obj,1))\n",
    "test_len = ytest.shape[0]\n",
    "tot_correct_count = 0\n",
    "\n",
    "for i in range(ytest.shape[0]):\n",
    "    b = np.argmax(ytest[i])\n",
    "    tot_count_obj[b] = tot_count_obj[b]+1\n",
    "    if(b==np.argmax(ytest_pred[i])):\n",
    "        tot_correct_count = tot_correct_count +1\n",
    "    else:\n",
    "        wrong_count_obj[b] = wrong_count_obj[b]+1\n",
    "print(f'In test dataset the total number of correct guessess is {tot_correct_count} out of {test_len}')\n",
    "print(f'score = {round(tot_correct_count/test_len,5)}')\n",
    "print('----------------')\n",
    "print('The % of being wrong for each object:\\n')\n",
    "for ob_idx,ob in enumerate(objects):\n",
    "    print(f'{ob} is guessed wrongly {wrong_count_obj[ob_idx][0]} times out of {tot_count_obj[ob_idx][0]} --> {round(100*wrong_count_obj[ob_idx][0]/tot_count_obj[ob_idx][0])}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
